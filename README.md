<h1 align="center">DeepSeek-R1:1.5B Local Chat ðŸš€</h1>

<p align="center">
  <em>Run the DeepSeek-R1:1.5B language model locally using Python and Ollama.</em>
</p>

---

## âœ¨ Features

- âš¡ Run the **DeepSeek-R1:1.5B** model completely offline.
- ðŸ› ï¸ Simple Python script to interact with the model.
- ðŸ“¦ Lightweight setup using a virtual environment.

---

## ðŸ“‹ Requirements

- ðŸ **Python** 3.8 or higher
- ðŸ§  **Ollama** installed and running
- ðŸ“¥ **DeepSeek-R1:1.5B** model pulled via Ollama

---

## âš™ï¸ Setup Instructions

### 1. Clone the Repository

https://github.com/Tawfiq-alqaidy/Run-DeepSeek-R1-1.5B-locally-using-Python-and-Ollama.git

### 2. Create and Activate a Virtual Environment

#### On Windows:
```bash
python -m venv .venv
.venv\Scripts\activate
```

#### On macOS/Linux:
```bash
python3 -m venv .venv
source .venv/bin/activate
```

### 3. Install Dependencies
```bash
pip install -r requirements.txt
```

### 4. Pull the DeepSeek-R1:1.5B Model
```bash
ollama pull deepseek-r1:1.5b
```

### 5. Run the Script
```bash
python main.py
```

> The script will send a predefined prompt to the model and display the response.

---

## ðŸ›  Notes

- Make sure **Ollama** is running before you execute the script.
- Feel free to customize `main.py` to change prompts or enhance functionality!



<p align="center">
  <img src="https://capsule-render.vercel.app/api?type=waving&color=0:4F46E5,100:22D3EE&height=120&section=footer" />
</p>

<p align="center">
  <b>Happy Chatting with DeepSeek! ðŸŽ‰</b>
</p>
```
